{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json           \n",
    "import random         \n",
    "import gzip            \n",
    "import requests        \n",
    "import torch           \n",
    "from peft import get_peft_model, LoraConfig, TaskType  \n",
    "from torch.utils.data import Dataset, DataLoader  \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  \n",
    "from torch.optim import AdamW    \n",
    "from tqdm import tqdm   \n",
    "import re               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(text):#creating the promt for emotion classificatiom\n",
    "    return f\"predict the meotion for the following text : {text}\\nEmotion:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_text(tokenizer, text, return_tensor = False):#encoding our text using the gpt2 tokenizer\n",
    "    if return_tensor:\n",
    "        return tokenizer.enocde(\n",
    "            text,add_special_tokens =False ,return_tensor =\"pt\"\n",
    "        )\n",
    "    else :\n",
    "        return tokenizer.encode(text,add_special_tokens=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoded_text(tokenizer,tokens_ids):#tto decode the id back to tokens\n",
    "    return tokenizer.decode(tokens_ids , skip_special_tokens = True)#by keeping skip_spl_tokjen true we remove all <eos><sod><pad>'s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCompletionDataset(Dataset):\n",
    "    def __init__(self,data,tokenizer):\n",
    "        self.data  = data\n",
    "        self.tokenizer = tokenizer \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        item=self.data[idx]\n",
    "        prompt = item[\"prompt\"]\n",
    "        completion = item[\"completion\"]\n",
    "        encoded_prompt = encoded_text(self.tokenizer , prompt)\n",
    "        encoded_completion = encoded_text(self.tokenizer, completion)\n",
    "        eos_token =self.tokenizer.eos_token_id\n",
    "        input_ids = encoded_prompt + encoded_completion + [eos_token]\n",
    "        labels =[-100]*len(encoded_prompt)+encoded_completion+[eos_token]\n",
    "        return{\n",
    "            \"input_ids\":input_ids,\n",
    "            \"labels\":labels,\n",
    "            \"prompt\":prompt,\n",
    "            \"expected_completion\":completion\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    #get max length string in the bactch\n",
    "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
    "    #padding the tokens\n",
    "    input_ids =[\n",
    "        item[\"input_ids\"]+ [tokenizer.pad_token_id]*(max_length-len(item[\"input_ids\"]))\n",
    "        for item in batch        \n",
    "    ]\n",
    "    labels =[\n",
    "        item[\"labels\"]+[-100]*(max_length - len(item[\"labels\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    attention_mask = [\n",
    "        [1]* len(item[\"input_ids\"])+\n",
    "        [0]* (max_length-len(item[\"input_ids\"]))\n",
    "        for item in batch\n",
    "    ]\n",
    "    prompts =[item[\"prompt\"] for item in batch]\n",
    "    expected_completions = [item[\"expected_completion\"]for item in batch]\n",
    "    return(\n",
    "        torch.tensor(input_ids, dtype=torch.long),\n",
    "        torch.tensor(attention_mask, dtype=torch.long),\n",
    "        torch.tensor(labels, dtype=torch.long),\n",
    "        prompts,\n",
    "        expected_completions\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, tokenizer, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels, prompts, expected_completions in loader:\n",
    "            for prompt, expected_completion in zip(prompts, expected_completions):\n",
    "                generated_text = generate_text(model, tokenizer, prompt)\n",
    "                if normalize_text(generated_text) == normalize_text(expected_completion):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    model.train()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids[\"input_ids\"],\n",
    "        attention_mask=input_ids[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )[0]\n",
    "    generated_text = decode_text(tokenizer, output_ids[input_ids[\"input_ids\"].shape[1]:])\n",
    "    return generated_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model(model_path,test_inputs):\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")    \n",
    "    print(f\"Using device: {device}\")\n",
    "    model =AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer =AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    prompt = build_prompt(test_input)\n",
    "    generated_text = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"Generated emotion: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_prepare_data(data_url,tokenizer,batch_size,test_ratio=0.1):\n",
    "    response = requests.get(data_url)\n",
    "    content = gzip.decompress(response.content).decode()\n",
    "    dataset = []\n",
    "    for entry in map(json.loads, content.splitlines()):\n",
    "        dataset.append({\n",
    "            \"prompt\": build_prompt(entry['text']),\n",
    "            \"completion\": entry[\"label\"].strip()\n",
    "        })\n",
    "    random.shuffle(dataset)\n",
    "    split_index = int(len(dataset) * (1 - test_ratio))\n",
    "    train_data = dataset[:split_index]\n",
    "    test_data = dataset[split_index:]\n",
    "    train_dataset = PromptCompletionDataset(train_data, tokenizer)\n",
    "    test_dataset = PromptCompletionDataset(test_data, tokenizer)\n",
    "    train_loader =DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        collate_fn =collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle =False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader ,test_loader\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    num_epochs=18\n",
    "    batch_size = 16\n",
    "    learning_rate = 5e-5\n",
    "    return num_epochs,batch_size,learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:24<00:00,  2.93it/s, Loss=0.0364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average loss: 0.0364, Test accuracy: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:49<00:00,  2.74it/s, Loss=0.0259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average loss: 0.0259, Test accuracy: 0.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [07:33<00:00,  2.48it/s, Loss=0.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average loss: 0.0222, Test accuracy: 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:45<00:00,  2.78it/s, Loss=0.0156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average loss: 0.0156, Test accuracy: 0.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:45<00:00,  2.77it/s, Loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average loss: 0.0129, Test accuracy: 0.9065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/18: 100%|██████████████████████████████████████████████████████████████████████| 1125/1125 [06:46<00:00,  2.77it/s, Loss=0.015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Average loss: 0.0150, Test accuracy: 0.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:45<00:00,  2.78it/s, Loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Average loss: 0.0119, Test accuracy: 0.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:45<00:00,  2.77it/s, Loss=0.0132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Average loss: 0.0132, Test accuracy: 0.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:43<00:00,  2.79it/s, Loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Average loss: 0.0125, Test accuracy: 0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:44<00:00,  2.78it/s, Loss=0.017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Average loss: 0.0170, Test accuracy: 0.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/18: 100%|█████████████████████████████████████████████████████████████████████| 1125/1125 [06:46<00:00,  2.77it/s, Loss=0.023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Average loss: 0.0230, Test accuracy: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:46<00:00,  2.77it/s, Loss=0.0323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Average loss: 0.0323, Test accuracy: 0.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:49<00:00,  2.75it/s, Loss=0.0396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Average loss: 0.0396, Test accuracy: 0.8790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:48<00:00,  2.75it/s, Loss=0.0455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Average loss: 0.0455, Test accuracy: 0.8670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:49<00:00,  2.75it/s, Loss=0.0528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Average loss: 0.0528, Test accuracy: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:48<00:00,  2.75it/s, Loss=0.0415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Average loss: 0.0415, Test accuracy: 0.8215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:47<00:00,  2.76it/s, Loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Average loss: 0.0437, Test accuracy: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/18: 100%|████████████████████████████████████████████████████████████████████| 1125/1125 [06:48<00:00,  2.75it/s, Loss=0.0401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Average loss: 0.0401, Test accuracy: 0.8590\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    data_url = \"https://www.thelmbook.com/data/emotions\"\n",
    "    model_name = \"openai-community/gpt2\"\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token= tokenizer.eos_token\n",
    "    peft_config =LoraConfig(\n",
    "        task_type = TaskType.CAUSAL_LM,\n",
    "        inference_mode = False,\n",
    "        r=16,\n",
    "        lora_alpha=32\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    num_epochs, batch_size, learning_rate = get_hyperparameters()\n",
    "    train_loader, test_loader = download_and_prepare_data(data_url, tokenizer, batch_size)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for input_ids, attention_mask, labels, _, _ in progress_bar:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix({\"Loss\": total_loss / num_batches})\n",
    "    \n",
    "        avg_loss = total_loss / num_batches\n",
    "        test_acc = calculate_accuracy(model, tokenizer, test_loader)\n",
    "        print(f\"Epoch {epoch+1} - Average loss: {avg_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_finetuned_gpt2/tokenizer_config.json',\n",
       " 'lora_finetuned_gpt2/special_tokens_map.json',\n",
       " 'lora_finetuned_gpt2/vocab.json',\n",
       " 'lora_finetuned_gpt2/merges.txt',\n",
       " 'lora_finetuned_gpt2/added_tokens.json',\n",
       " 'lora_finetuned_gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_finetuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"lora_finetuned_gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "--- Generated Output ---\n",
      "I am so happy today!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_model(\"lora_finetuned_gpt2\", \"I am so happy today!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (german2english)",
   "language": "python",
   "name": "german2english"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
