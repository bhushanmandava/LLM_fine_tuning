{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch                             # PyTorch for tensor operations and deep learning\n",
    "import torch.nn as nn                    # Neural network module from PyTorch\n",
    "import numpy as np                       # NumPy for numerical operations\n",
    "import re                                # Regular expressions for text processing (if needed)\n",
    "import urllib.request                    # For downloading files from URLs\n",
    "import gzip                              # For handling compressed files\n",
    "import json                              # For parsing JSON data\n",
    "import requests                          # For making HTTP requests to download data\n",
    "import random                            # For shuffling data and setting random seeds\n",
    "import pickle                            # For saving and loading serialized objects\n",
    "import os                                # For file system operations\n",
    "from tqdm import tqdm                    # For displaying progress bars during loops\n",
    "from torch.utils.data import Dataset, DataLoader  # For creating custom datasets and data loaders in PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, embeddings, em_dim, seq_len):\n",
    "        self.embeddings = embeddings\n",
    "        self.em_dim = em_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def embed(self, tokens):\n",
    "        embeddings = []\n",
    "        for word in tokens[:self.seq_len]:\n",
    "            if word in self.embeddings:\n",
    "                embeddings.append(torch.tensor(self.embeddings[word]))\n",
    "            elif word.lower() in self.embeddings:\n",
    "                embeddings.append(torch.tensor(self.embeddings[word.lower()]))\n",
    "            else:\n",
    "                embeddings.append(torch.zeros(self.em_dim))\n",
    "        if len(embeddings) < self.seq_len:\n",
    "            padding_size = self.seq_len - len(embeddings)\n",
    "            embeddings.extend([torch.zeros(self.em_dim)] * padding_size)\n",
    "        return torch.stack(embeddings)\n",
    "\n",
    "def load_embeddings(url, filename=\"vectors.dat\"):\n",
    "    if not os.path.exists(filename):\n",
    "        with tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024, desc=\"Downloading\") as progress_bar:\n",
    "            def report_hook(count, block_size, total_size):\n",
    "                if total_size != -1:\n",
    "                    progress_bar.total = total_size\n",
    "                progress_bar.update(block_size)\n",
    "            urllib.request.urlretrieve(url, filename, reporthook=report_hook)\n",
    "    else:\n",
    "        print(f\"File {filename} already exists. Skipping download.\")\n",
    "\n",
    "    with gzip.open(filename, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, emb_dim = map(int, header.split())\n",
    "        vectors = {}\n",
    "        binary_len = np.dtype(\"float32\").itemsize * emb_dim\n",
    "        with tqdm(total=vocab_size, desc=\"Loading word vectors\") as pbar:\n",
    "            for _ in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == b\" \":\n",
    "                        word = b\"\".join(word).decode(\"utf-8\")\n",
    "                        break\n",
    "                    if ch != b\"\\n\":\n",
    "                        word.append(ch)\n",
    "                vector = np.frombuffer(f.read(binary_len), dtype=\"float32\")\n",
    "                vectors[word] = vector\n",
    "                pbar.update(1)\n",
    "    return vectors, emb_dim\n",
    "\n",
    "def load_and_split_data(url, test_ratio=0.1):\n",
    "    response = requests.get(url)\n",
    "    content = gzip.decompress(response.content).decode()\n",
    "    data = [json.loads(line) for line in content.splitlines() if line.strip()]  # Ensure valid JSON objects\n",
    "    random.shuffle(data)\n",
    "    split_index = int(len(data) * (1 - test_ratio))\n",
    "    return data[:split_index], data[split_index:]\n",
    "\n",
    "def download_and_prepare_data(data_url, vectors_url, seq_len, batch_size):\n",
    "    train_split, test_split = load_and_split_data(data_url, test_ratio=0.1)\n",
    "    embeddings, emb_dim = load_embeddings(vectors_url)\n",
    "    label_to_id, id_to_label, num_classes = create_label_mappings(train_split)\n",
    "    tokenizer = Tokenizer()\n",
    "    embedder = Embedder(embeddings, emb_dim, seq_len)\n",
    "    train_loader, test_loader = create_data_loaders(\n",
    "        train_split, test_split,\n",
    "        tokenizer, embedder,\n",
    "        label_to_id, batch_size\n",
    "    )\n",
    "    return (train_loader, test_loader, id_to_label, num_classes, emb_dim)\n",
    "\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, embedder, label_to_id):\n",
    "        self.texts = [item[\"text\"] for item in data]\n",
    "        self.label_ids = [label_to_id[item[\"label\"]] for item in data]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer.tokenize(self.texts[idx])\n",
    "        embeddings = self.embedder.embed(tokens)\n",
    "        return embeddings, torch.tensor(self.label_ids[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, seq_len, id_to_label):\n",
    "        super().__init__()\n",
    "        self.config = {\n",
    "            \"emb_dim\": emb_dim,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"id_to_label\": id_to_label\n",
    "        }\n",
    "        self.conv1 = nn.Conv1d(emb_dim, 512, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(512, 256, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * seq_len, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------- Utility Functions -------------------- #\n",
    "\n",
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            embeddings, label = batch\n",
    "            embeddings = embeddings.to(device)\n",
    "            label = label.to(device)\n",
    "            logits = model(embeddings)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "    accuracy = correct / total\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "def create_label_mappings(train_dataset):\n",
    "    unique_labels = sorted(set(item[\"label\"] for item in train_dataset))\n",
    "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "    return label_to_id, id_to_label, len(unique_labels)\n",
    "\n",
    "def create_data_loaders(train_split, test_split, tokenizer, embedder, label_to_id, batch_size):\n",
    "    train_dataset = TextClassificationDataset(train_split, tokenizer, embedder, label_to_id)\n",
    "    test_dataset = TextClassificationDataset(test_split, tokenizer, embedder, label_to_id)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def save_model(model, prefix):\n",
    "    torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"config\": model.config\n",
    "    }, f\"{prefix}_model.pth\")\n",
    "\n",
    "def load_model(prefix):\n",
    "    checkpoint = torch.load(f\"{prefix}_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "    config = checkpoint[\"config\"]\n",
    "    model = CNNTextClassifier(\n",
    "        emb_dim=config[\"emb_dim\"],\n",
    "        num_classes=config[\"num_classes\"],\n",
    "        seq_len=config[\"seq_len\"],\n",
    "        id_to_label=config[\"id_to_label\"]\n",
    "    )\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_input, tokenizer=None, embedder=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer()\n",
    "    if embedder is None:\n",
    "        embeddings, emb_dim = load_embeddings(vectors_url)\n",
    "        embedder = Embedder(embeddings, emb_dim, model.config[\"seq_len\"])\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.tokenize(test_input)\n",
    "        embeddings = embedder.embed(tokens).unsqueeze(0).to(device)\n",
    "        outputs = model(embeddings)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_label = model.config[\"id_to_label\"][predicted.item()]\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"Predicted emotion: {predicted_label}\")\n",
    "\n",
    "def set_hyperparameters():\n",
    "    num_epochs = 2\n",
    "    seq_len = 100\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    return num_epochs, seq_len, batch_size, learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "File vectors.dat already exists. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading word vectors: 100%|██████████████████████████████████████████████████████████████| 3000000/3000000 [00:18<00:00, 160181.55it/s]\n",
      "Epoch 1/2: 100%|█████████████████████████████████████████████████████████████████████████| 563/563 [00:15<00:00, 36.06it/s, Loss=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Test Accuracy: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|█████████████████████████████████████████████████████████████████████████| 563/563 [00:12<00:00, 44.44it/s, Loss=0.247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Test Accuracy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set_seed(42)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_url = \"https://www.thelmbook.com/data/emotions\"\n",
    "    vectors_url = \"https://www.thelmbook.com/data/word-vectors\"\n",
    "\n",
    "    num_epochs, seq_len, batch_size, learning_rate = set_hyperparameters()\n",
    "\n",
    "    train_loader, test_loader, id_to_label, num_classes, emb_dim = \\\n",
    "        download_and_prepare_data(data_url, vectors_url, seq_len, batch_size)\n",
    "\n",
    "    model = CNNTextClassifier(emb_dim, num_classes, seq_len, id_to_label)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            batch_embeddings, batch_labels = batch\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix({\"Loss\": total_loss / num_batches})\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        test_acc = calculate_accuracy(model, test_loader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    model_name = \"CNN_classifier\"\n",
    "    save_model(model, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File vectors.dat already exists. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading word vectors: 100%|██████████████████████████████████████████████████████████████| 3000000/3000000 [00:18<00:00, 159378.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I'm so happy to be able to train a text classifier!\n",
      "Predicted emotion: joy\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loaded_model = load_model(model_name)\n",
    "    embeddings, emb_dim = load_embeddings(vectors_url)\n",
    "    tokenizer = Tokenizer()\n",
    "    embedder = Embedder(embeddings, emb_dim, seq_len)\n",
    "    test_input = \"I'm so happy to be able to train a text classifier!\"\n",
    "    test_model(loaded_model, test_input, tokenizer, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (german2english)",
   "language": "python",
   "name": "german2english"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
